{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOPP 2019W Exercise 3 - Group 32\n",
    "\n",
    "### Contributors\n",
    "- Eszter Katalin Bognar - 11931695\n",
    "- Luis Kolb - 01622731\n",
    "- Alexander Leitner - 01525882\n",
    "\n",
    "### Objectives of the analysis\n",
    "- What is the most accurate overview of flows of refugees between countries that can be obtained? \n",
    "- Are there typical characteristics of refugee origin and destination countries? \n",
    "- Are there typical characteristics of large flows of refugees? \n",
    "- Can countries that will produce large numbers of refugees be predicted? Can refugee flows be predicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import hashlib\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "We started our analysis by loading the necessary data files.\n",
    "\n",
    "We selected 4 datasets to use:\n",
    "- OECD International Migration Database data (https://stats.oecd.org/Index.aspx?DataSetCode=MIG)\n",
    "- Gross Domestic Product per Capita data (https://data.worldbank.org/indicator/NY.GDP.PCAP.CD)\n",
    "- Human Development Index data (http://hdr.undp.org/en/data)\n",
    "- World Governance Index data (https://datacatalog.worldbank.org/dataset/worldwide-governance-indicators)\n",
    "\n",
    "Each dataset were loaded and formatted including:\n",
    "\n",
    "- reshaping columns (changing rows to columns or columns to rows when necessary),\n",
    "- getting rid of unwanted columns, \n",
    "- renaming columns, \n",
    "- setting proper data types,\n",
    "- setting country-year multiindex to facilitate future data merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load & format OECD International Migration Database data\n",
    "'''\n",
    "\n",
    "def load_oecd_data():\n",
    "    \"\"\" \n",
    "    Load oecd data file\n",
    "    Reshape the dataset to have country, destination, year, asylum_seekers columns\n",
    "    Set hierarchical index (country, year)\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    oecd: data frame containing oecd data\n",
    "    \"\"\"\n",
    "    #load dataset\n",
    "    oecd = pd.read_csv('data/oecd_data.csv',na_values=['..'])\n",
    "    #reshape table\n",
    "    oecd.set_index(['country','destination', 'year','variable'], inplace=True)\n",
    "    oecd=oecd.unstack()\n",
    "    oecd.columns = oecd.columns.droplevel(0)\n",
    "    #drop unwanted columns\n",
    "    oecd.reset_index(drop=False, inplace=True)\n",
    "    #rename columns\n",
    "    oecd = oecd[['country','destination','year','Inflows of asylum seekers by nationality']]\n",
    "    oecd = oecd.rename(columns={'Inflows of asylum seekers by nationality': 'asylum_seekers'})\n",
    "    #set index\n",
    "    oecd=oecd.set_index(['country', 'year'])\n",
    "    return oecd\n",
    "\n",
    "oecd_df=load_oecd_data()\n",
    "oecd_df.info()\n",
    "oecd_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load & format Gross Domestic Product per Capita data\n",
    "'''\n",
    "\n",
    "def load_gdp_data():\n",
    "    \"\"\" \n",
    "    Load gdp data file\n",
    "    Reshape the dataset to have country, year, GDP columns\n",
    "    Change data types\n",
    "    Set hierarchical index (country, year)\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    gdp: data frame containing gdp data\n",
    "    \"\"\"\n",
    "    #load dataset\n",
    "    gdp = pd.read_csv('data/GDPPC_data.csv',na_values=['..'])\n",
    "    #drop unwanted columns\n",
    "    gdp.drop(['Indicator Name','Indicator Code','Country Code' ], inplace=True, axis=1)\n",
    "    #reshape dataframe\n",
    "    gdp.set_index(['Country Name'], inplace=True)\n",
    "    gdp=gdp.stack(dropna=False).to_frame()\n",
    "    gdp.reset_index(drop=False, inplace=True)\n",
    "    #rename columns\n",
    "    gdp=gdp.rename(columns={'Country Name': 'country', 'level_1': 'year', 0: 'GDP'})\n",
    "    #set datatype for year, GDP\n",
    "    gdp['year']=gdp['year'].astype(int)\n",
    "    gdp['GDP']=gdp['GDP'].astype(float)\n",
    "    #only use data from 2000 onwards\n",
    "    gdp = gdp[gdp.year >= 2000]\n",
    "    #set index\n",
    "    gdp=gdp.set_index(['country', 'year'])\n",
    "    return gdp\n",
    "\n",
    "gdp_df=load_gdp_data()\n",
    "gdp_df.info()\n",
    "gdp_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load & format Human Development Index data\n",
    "'''\n",
    "\n",
    "def load_hdi_data():\n",
    "    \"\"\" \n",
    "    Load hdi data file\n",
    "    Reshape the dataset to have country, year, HDI columns\n",
    "    Change data types\n",
    "    Set hierarchical index (country, year)\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    hdi: data frame containing hdi data\n",
    "    \"\"\"\n",
    "    #load dataset\n",
    "    hdi = pd.read_csv('data/HDI.csv',na_values=['..'])\n",
    "    #drop unwanted columns\n",
    "    hdi.drop(['HDI Rank (2018)'], inplace=True, axis=1)\n",
    "    #reshape dataframe\n",
    "    hdi.set_index(['Country'], inplace=True)\n",
    "    hdi=hdi.stack(dropna=False).to_frame()\n",
    "    hdi.reset_index(drop=False, inplace=True)\n",
    "    #rename columns\n",
    "    hdi=hdi.rename(columns={'Country': 'country', 'level_1': 'year', 0: 'HDI'})\n",
    "    #set datatype for year\n",
    "    hdi['year']=hdi['year'].astype(int)\n",
    "    hdi['HDI']=hdi['HDI'].astype(float)\n",
    "    #only use data from 2000 onwards\n",
    "    hdi = hdi[hdi.year >= 2000]\n",
    "    #set index\n",
    "    hdi=hdi.set_index(['country', 'year'])\n",
    "    return hdi\n",
    "\n",
    "hdi_df=load_hdi_data()\n",
    "hdi_df.info()\n",
    "hdi_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load & format World Governance Index data\n",
    "\n",
    "column values:\n",
    "CC.EST: Control of Corruption: Estimate\n",
    "GE.EST: Government Effectiveness: Estimate\n",
    "PV.EST: Political Stability and Absence of Violence/Terrorism: Estimate\n",
    "RL.EST: Rule of Law: Estimate\n",
    "RQ.EST: Regulatory Quality: Estimate\n",
    "VA.EST: Voice and Accountability: Estimate\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def load_wgi_data():\n",
    "    \"\"\" \n",
    "    Load wgi data file\n",
    "    Reshape the dataset to have country, year, WGI columns\n",
    "    Change data types\n",
    "    Set hierarchical index (country, year)\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    hdi: data frame containing hdi data\n",
    "    \"\"\"\n",
    "    #load dataset\n",
    "    wgi = pd.read_csv('data/WGIData.csv',na_values=['..'])\n",
    "    #drop unwanted rows\n",
    "    wgi=wgi[wgi['Indicator Code'].str.contains('EST', regex= True, na=False)]\n",
    "    #drop unwanted columns\n",
    "    wgi.drop(['Country Code','Indicator Name'], inplace=True, axis=1)\n",
    "    #reshape dataframe\n",
    "    wgi.set_index(['Country Name','Indicator Code'], inplace=True)\n",
    "    wgi=wgi.stack(dropna=False).to_frame()\n",
    "    wgi.reset_index(drop=False, inplace=True)\n",
    "    #rename columns\n",
    "    wgi=wgi.rename(columns={'Country Name': 'country', 'level_2': 'year', 0: 'variable'})\n",
    "    wgi.set_index(['country','year','Indicator Code'], inplace=True)\n",
    "    wgi=wgi.unstack()\n",
    "    wgi.columns = wgi.columns.droplevel(0)\n",
    "    wgi.reset_index(drop=False, inplace=True)\n",
    "    #set datatype for year \n",
    "    wgi['year']=wgi['year'].astype(int)\n",
    "    #only use data from 2000 onwards\n",
    "    wgi = wgi[wgi.year >= 2000]\n",
    "    #set index\n",
    "    wgi=wgi.set_index(['country', 'year'])\n",
    "    return wgi\n",
    "wgi_df=load_wgi_data()\n",
    "wgi_df.info()\n",
    "wgi_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country name inconsistency check \n",
    "Before data merge we have to check the datasets for inconsistencies. \n",
    "We would like to merge on the country-year multiindex.\n",
    "Year is consistent in each data files however we have to search for different usage and typos in county names.\n",
    "We selected oecd_df as base dataframe so we compare the country names in the oecd_df to the country names in the hdi_df, gdp_df, wgi_df datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def country_check(countries, gdp_df, hdi_df, wgi_df):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    --------\n",
    "    countries: list of countries (source or destination) in oecd_df\n",
    "    gdp_df: gdp of the countries\n",
    "    hdi_df: hdi of the countries\n",
    "    wgi_df: wgi data of the countries\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    check: dataframe showing inconsistent country name usage\n",
    "    \"\"\"   \n",
    "    #get country name list series\n",
    "    hdi_c = hdi_df.index.unique(level=0).to_series()\n",
    "    gdp_c = gdp_df.index.unique(level=0).to_series()\n",
    "    wgi_c = wgi_df.index.unique(level=0).to_series()\n",
    "    \n",
    "    #create dataframe for the results\n",
    "    check=pd.DataFrame(columns=['country','hdi','gdp','wgi'])\n",
    "    #iterate through oecd_df source or destination country names and check presence of country name in other dfs\n",
    "    for index,row in countries.iterrows():\n",
    "        check=check.append({'country': row.country,\n",
    "                            'hdi':row.isin(hdi_c).values[0],\n",
    "                           'gdp':row.isin(gdp_c).values[0],\n",
    "                           'wgi':row.isin(wgi_c).values[0]}, ignore_index=True)\n",
    "        \n",
    "    #add only problematic country rows to the result df\n",
    "    check=check.loc[(check['hdi'] == False) | (check['gdp'] == False) | (check['wgi'] == False)]\n",
    "    return check\n",
    "\n",
    "oecd_s = pd.DataFrame(oecd_df.index.unique(level=0))\n",
    "oecd_s_check = country_check(oecd_s,gdp_df, hdi_df, wgi_df)\n",
    "\n",
    "oecd_d = pd.DataFrame(oecd_df['destination'].unique(),columns={'country'})\n",
    "oecd_d_check = country_check(oecd_d,gdp_df, hdi_df, wgi_df)\n",
    "\n",
    "display(oecd_d_check.head(50),oecd_s_check.head(50))\n",
    "df = pd.concat([oecd_d_check,oecd_s_check])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making the country names consistent, we first tried out the fuzzy search method of the fuzzywuzzy library. \n",
    "Due to errors, we finally decided to manually create a dictonary of country names to replace or delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dict = {\n",
    "    'Bahamas, The' :\"Bahamas\",\n",
    "    'Bolivia (Plurinational State of)' :\"Bolivia\",\n",
    "    'Cabo Verde' :\"Cape Verde\",\n",
    "    'Congo, Rep.' :\"Congo\",\n",
    "    'Czechia' :\"Czech Republic\",\n",
    "    \"Cote d'Ivoire\" :\"Côte d'Ivoire\",\n",
    "    'Congo, Dem. Rep.' :\"Democratic Republic of the Congo\",\n",
    "    'Congo (Democratic Republic of the)':\"Democratic Republic of the Congo\",\n",
    "    'Egypt, Arab Rep.' :\"Egypt\",\n",
    "    'Gambia, The' :\"Gambia\",\n",
    "    'Iran (Islamic Republic of)' :\"Iran\",\n",
    "    'Iran, Islamic Rep.': \"Iran\",\n",
    "    'Korea (Republic of)' :\"Korea\",\n",
    "    'Korea, Rep.':\"Korea\",\n",
    "    'Kyrgyz Republic' :\"Kyrgyzstan\",\n",
    "    \"Lao People's Democratic Republic\" :\"Laos\",\n",
    "    \"Lao PDR\":\"Laos\",\n",
    "    'Micronesia (Federated States of)' :\"Micronesia\",\n",
    "    'Micronesia, Fed. Sts.':'Micronesia',\n",
    "    'Moldova (Republic of)' :\"Moldova\",\n",
    "    'Russian Federation' :\"Russia\",\n",
    "    'St. Kitts and Nevis' :\"Saint Kitts and Nevis\",\n",
    "    'St. Lucia' :\"Saint Lucia\",\n",
    "    'St. Vincent and the Grenadines' :\"Saint Vincent and the Grenadines\",\n",
    "    'Slovakia' :\"Slovak Republic\",\n",
    "    'Syrian Arab Republic' :\"Syria\",\n",
    "    'Eswatini' :\"Swaziland\",\n",
    "    \"Eswatini (Kingdom of)\": \"Swaziland\",\n",
    "    'Tanzania (United Republic of)' :\"Tanzania\",\n",
    "    'Venezuela (Bolivarian Republic of)' :\"Venezuela\",\n",
    "    \"Venezuela, RB\":\"Venezuela\",\n",
    "    'Vietnam' :\"Viet Nam\",\n",
    "    'Yemen, Rep.' :\"Yemen\"  \n",
    "}\n",
    "\n",
    "country_del = [\n",
    "    #former countries\n",
    "    \"Former Czechoslovakia\",\n",
    "    \"Former USSR\",\n",
    "    \"Former Yugoslavia\",\n",
    "    \"Serbia and Montenegro\",\n",
    "    #China' territory\n",
    "    \"Macau\",\n",
    "    \"Chinese Taipei\",\n",
    "    \"Hong Kong, China\",\n",
    "    #USA's territory \n",
    "    \"Guam\",\n",
    "    \"Puerto Rico\",\n",
    "    #GB' territory\n",
    "    \"Bermuda\",\n",
    "    #New Zealand' territory\n",
    "    \"Cook Islands\",\n",
    "    \"Tokelau\",\n",
    "    \"Niue\",\n",
    "    #Palestina's territory\n",
    "    \"West Bank and Gaza Strip\",\n",
    "    #no proper data\n",
    "    \"Nauru\",\n",
    "    \"San Marino\",\n",
    "    \"Somalia\",\n",
    "    \"Tuvalu\",\n",
    "    \"Democratic People's Republic of Korea\",\n",
    "    \"Monaco\",\n",
    "    \"Not stated\",\n",
    "    \"Unknown\",\n",
    "    \"Total\",\n",
    "]\n",
    "\n",
    "def country_correction(df):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    --------\n",
    "    df: dataframe with original country names\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    df_corrected: data frame with replaced country names\n",
    "    \"\"\"\n",
    "    #rename country names  \n",
    "    df_corrected=df.rename(index=country_dict, level=0)\n",
    "    return df_corrected\n",
    "\n",
    "#delete unwanted countries from the oecd_df base df\n",
    "oecd_df=oecd_df.drop(country_del, level=0, errors='ignore')\n",
    "\n",
    "#call country_correction() to rename country names\n",
    "hdi_df=country_correction(hdi_df)\n",
    "gdp_df=country_correction(gdp_df)\n",
    "wgi_df=country_correction(wgi_df)\n",
    "\n",
    "\n",
    "#check for inconsistencies again\n",
    "countries = pd.DataFrame(oecd_df.index.unique(level=0))\n",
    "oecd_check = country_check(countries,gdp_df, hdi_df, wgi_df)\n",
    "\n",
    "if oecd_check.empty:\n",
    "    print(\"No more inconsistent country names!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection\n",
    "We checked the distribution of hdi, wgi and gdp data showing there are more poor than wealthy countries...We can not see any outliers. GDP is skewed towards zero, HDI is in the range of [0-1], wgi metrics are in the range of [-2-2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_df['GDP'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdi_df['HDI'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgi_df[['CC.EST','GE.EST','PV.EST','RL.EST','VA.EST']].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values\n",
    "\n",
    "### hdi, gdp and wgi datasets\n",
    "After resolving the country name inconsistency and outlier check, we moved forward to examine outliers and missing values in the data files. We started with the analysis of the datasets to identify problematic areas, then implemented our solution to handle them. \n",
    "At this point we only focus on the hdi, gdp and wgi datasets since these datasets can be treated similarly. \n",
    "For the oecd dataset we will use other methods later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#investigating occurences of missing values in the datasets\n",
    "display(hdi_df.isnull().sum())\n",
    "display(gdp_df.isnull().sum())\n",
    "display(wgi_df.isnull().sum())\n",
    "#display a sample with missing values\n",
    "display(hdi_df.iloc[hdi_df.index.get_level_values('country') == 'Eritrea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the hdi, gdp and wgi indicators can be treated equally and the values don't change rapidly from year to year we replace the missing data with the median of the data for the given country pairs. We selected this method because interpolation can't work properly where there are many missing values one after another. Where there weren't any data available for the given country pairs, we simply dropped the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is a bit slow, please be patient!\n",
    "def handle_missingMetricValues(incomplete_data):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    --------\n",
    "    incomplete_data: data frame containing missing values \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    complete_data: data frame not containing any missing values\n",
    "    \"\"\"\n",
    "    #if possible try to fill missing values with interpolation\n",
    "    for i in incomplete_data.index.unique(level=0):\n",
    "        columns=incomplete_data[incomplete_data.index.get_level_values('country')==i].columns\n",
    "        for col in columns:\n",
    "            #fill missing values with mean\n",
    "            median=incomplete_data.loc[incomplete_data.index.get_level_values('country') == i][col].median()\n",
    "            incomplete_data.loc[(incomplete_data.index.get_level_values('country') == i) & (incomplete_data[col].isnull()), col] = median            \n",
    "    #drop rows where there isn't any data, sum can't be calculated\n",
    "    complete_data=incomplete_data.dropna()\n",
    "    return complete_data\n",
    "\n",
    "hdi_complete=handle_missingMetricValues(hdi_df)\n",
    "gdp_complete=handle_missingMetricValues(gdp_df)\n",
    "wgi_complete=handle_missingMetricValues(wgi_df)\n",
    "\n",
    "display(hdi_complete.isnull().sum())\n",
    "display(gdp_complete.isnull().sum())\n",
    "display(wgi_complete.isnull().sum())\n",
    "\n",
    "display(hdi_complete.iloc[hdi_complete.index.get_level_values('country') == 'Eritrea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values in the oecd dataset\n",
    "Examine the dataset and occurences of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(oecd_df.info())\n",
    "display(oecd_df.isnull().sum())\n",
    "#examining the number of people flows for given source-destination country pairs\n",
    "oecd=oecd_df.reset_index()\n",
    "#change year to string to avoid aggregation by groupby\n",
    "oecd['year']=oecd['year'].astype(str)\n",
    "#set multiindex (country-destination)\n",
    "oecd=oecd.set_index(['country','destination'])\n",
    "#sum the number of asylum_seekers for each country-destination pairs\n",
    "agg_df=oecd.groupby(oecd.index).sum(min_count=1)\n",
    "display(agg_df)\n",
    "#identify the country pairs where flows of people between the countries is zero\n",
    "missing_pairs=agg_df.loc[(agg_df['asylum_seekers'] == 0) |(agg_df['asylum_seekers'].isnull())]\n",
    "#list the missing country pairs \n",
    "display(missing_pairs)\n",
    "display(oecd.iloc[(oecd.index.get_level_values('country') == 'Albania') & (oecd.index.get_level_values('destination') == 'Chile')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of country pairs (e.g. Albania-Chile) where none of the years have inflows of asylum seekers between countries. Since we could not find any similar data source where there was appropiate data available for cold deck inputation. We assume that migration is not considerable between these countries and we decided to delete these rows from the final dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is a bit slow, please be patient!\n",
    "def delete_missingOECDValues(incomplete_data, missing_p):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    --------\n",
    "    incomplete_data: data frame containing missing values \n",
    "    missing_p: list of country pairs with missing data\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    complete_data: data frame with deleted missing values\n",
    "    \"\"\"\n",
    "    #set country-destination multiindex on the dataset\n",
    "    incomplete_data=incomplete_data.reset_index()\n",
    "    incomplete_data=incomplete_data.set_index(['country','destination'])\n",
    "    #delete rows where none of the years have inflows of asylum seekers between the given country pairs\n",
    "    for i in incomplete_data.index.unique():\n",
    "        if np.any(i==missing_p.index):\n",
    "            incomplete_data.drop(i,inplace=True)\n",
    "    complete_data=incomplete_data\n",
    "    return complete_data\n",
    "oecd_deleted=delete_missingOECDValues(oecd_df, missing_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine whether delete was successful or not\n",
    "check=oecd_deleted.reset_index()\n",
    "if check[(check['country']=='Albania') & (check['destination']=='Chile')].empty:\n",
    "    display('Delete successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining missing values, we calculated the mean of asylum_seekers for the given country pairs and filled the holes with this value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_missingOECDValues(incomplete_data):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    --------\n",
    "    incomplete_data: data frame containing missing values \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    complete_data: data frame with inputted missing values, final dataframe that won't have missing values anymore\n",
    "    \"\"\"\n",
    "    #set country-destination multiindex on the dataset\n",
    "    incomplete_data=incomplete_data.reset_index()\n",
    "    incomplete_data=incomplete_data.set_index(['country','destination'])\n",
    "    #input missing values with median \n",
    "    for i in incomplete_data.index.unique():\n",
    "        mean=incomplete_data.loc[(incomplete_data.index.get_level_values('country')==i[0]) & (incomplete_data.index.get_level_values('destination')==i[1])].mean()\n",
    "        incomplete_data.loc[(incomplete_data.index.get_level_values('country') == i[0]) & (incomplete_data.index.get_level_values('destination')==i[1]) & (incomplete_data['asylum_seekers'].isnull()), 'asylum_seekers'] = mean.asylum_seekers\n",
    "    complete_data=incomplete_data\n",
    "    return complete_data\n",
    "oecd_complete=input_missingOECDValues(oecd_deleted)\n",
    "display(oecd_complete.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set back country-year as index before data merge\n",
    "oecd_complete=oecd_complete.reset_index()\n",
    "oecd_complete=oecd_complete.set_index(['country','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dest(s_merged, gdp_df, hdi_df, wgi_df):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    --------\n",
    "    s_merged: merged dataset with values for the source country\n",
    "    gdp_df: gdp of the countries\n",
    "    hdi_df: hdi of the countries\n",
    "    wgi_df: wgi data of the countries\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    d_merged: merged data frame that contains complete migration and country data for the source and destination countries\n",
    "    \"\"\"\n",
    "    s_merged = s_merged.rename(columns={'destination': 'country'})\n",
    "    s_merged=s_merged.set_index(['year','country'])\n",
    "    merge1=pd.merge(gdp_df, hdi_df, on=['year', 'country'])\n",
    "    merge2=pd.merge(merge1, wgi_df, on=['year', 'country'])\n",
    "    d_merged=pd.merge(merge2,s_merged,on=['year','country'])\n",
    "    d_merged=d_merged.reset_index()\n",
    "    d_merged = d_merged.rename(columns={'GDP': 'd_GDP',\n",
    "                                             'HDI': 'd_HDI',\n",
    "                                             'CC.EST': 'd_CC.EST',\n",
    "                                             'GE.EST': 'd_GE.EST',\n",
    "                                             'PV.EST': 'd_PV.EST',\n",
    "                                             'RL.EST': 'd_RL.EST',\n",
    "                                             'VA.EST': 'd_VA.EST',\n",
    "                                             'RQ.EST': 'd_RQ.EST',\n",
    "                                             'country': 'destination'})\n",
    "    return d_merged\n",
    "\n",
    "\n",
    "def merge_data(oecd_df, gdp_df, hdi_df, wgi_df):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    --------\n",
    "    oecd_df: yearly data for foreign population inflow and asylum seeker inflow from source to destination country\n",
    "    gdp_df: gdp of the countries\n",
    "    hdi_df: hdi of the countries\n",
    "    wgi_df: wgi data of the countries\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    merged_data: merged data frame that contains complete migration and country data\n",
    "    \n",
    "    \"\"\"\n",
    "    merge1=pd.merge(gdp_df, hdi_df, on=['year', 'country'])\n",
    "    merge2=pd.merge(merge1, wgi_df, on=['year', 'country'])\n",
    "    s_merged=pd.merge(merge2,oecd_df,on=['year','country'])\n",
    "    s_merged=s_merged.reset_index()\n",
    "    s_merged = s_merged.rename(columns={'GDP': 's_GDP',\n",
    "                                             'HDI': 's_HDI',\n",
    "                                             'CC.EST': 's_CC.EST',\n",
    "                                             'GE.EST': 's_GE.EST',\n",
    "                                             'PV.EST': 's_PV.EST',\n",
    "                                             'RL.EST': 's_RL.EST',\n",
    "                                             'VA.EST': 's_VA.EST',\n",
    "                                             'RQ.EST': 's_RQ.EST',\n",
    "                                             'country': 'source'})\n",
    "    merged_data=merge_dest(s_merged,gdp_df, hdi_df, wgi_df)\n",
    "    #set multiindex with year, source, destination\n",
    "    merged_data=merged_data.set_index(['year','source'])\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "data_merged = merge_data(oecd_complete, gdp_complete, hdi_complete, wgi_complete)\n",
    "data_merged.to_csv(\"merged.csv\")\n",
    "data_merged.info()\n",
    "data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_merged.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "In this section we try to predict the refugee flow and see if this is possible. We use the liniar regression to do that. \n",
    "\n",
    "1) the merge data is allready preparet and are ready for further calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = []   # creat a list to store the names of the countries who would like to predict the refugies flow\n",
    "countries = [\"Chad\"]   # in this case the prediction of the country Chad\n",
    "data_merged=data_merged.reset_index()   # to reshape the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to prepare the data set for the prediction\n",
    "# input name of the country \n",
    "# output data target from the prediction and the datas from 2000 to 2016\n",
    "def load_ref_data_predict(countries):\n",
    "    # column names who will conentrate on\n",
    "\n",
    "    # set which colums we want to use for the prediction\n",
    "    columns = [\"year\",\"destination\",\"source\",\"d_VA.EST\",\"d_GDP\",\"d_HDI\",\"d_CC.EST\",\"d_GE.EST\",\"d_PV.EST\",\"d_RL.EST\",\"s_GDP\",\"s_HDI\",\"s_GE.EST\",\"s_PV.EST\",\"s_RL.EST\",\"s_RL.EST\",\"s_VA.EST\"] \n",
    "    ref_data_pred = data[data.year == 2017] # data set from only the year of 2017 \n",
    "    ref_data = data_merged[data.year != 2017] # data set from 2000 to 2016     \n",
    "    ref_data = ref_data[ref_data[\"source\"] == countries] # set the datas to the country the user like\n",
    "    ref_data_pred = ref_data_pred[ref_data_pred[\"source\"] == countries] # set the datas to the country the user like  \n",
    "    # set the name from the destination and sorce to numbers this is important for the prediction\n",
    "    ref_data['source'] = ref_data['source'].apply(lambda x:float(int(hashlib.sha1(x.encode('utf-8')).hexdigest(), 16)))\n",
    "    ref_data['destination'] = ref_data['destination'].apply(lambda x:float(int(hashlib.sha1(x.encode('utf-8')).hexdigest(), 16)))   \n",
    "    ref_data_pred['source'] = ref_data_pred['source'].apply(lambda x:float(int(hashlib.sha1(x.encode('utf-8')).hexdigest(), 16)))\n",
    "    ref_data_pred['destination'] = ref_data_pred['destination'].apply(lambda x:float(int(hashlib.sha1(x.encode('utf-8')).hexdigest(), 16)))\n",
    "    \n",
    "    return ref_data_pred,ref_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "columns = [\"year\",\"destination\",\"source\",\"d_VA.EST\",\"d_GDP\",\"d_HDI\",\"d_CC.EST\",\"d_GE.EST\",\"d_PV.EST\",\"d_RL.EST\",\"s_GDP\",\"s_HDI\",\"s_GE.EST\",\"s_PV.EST\",\"s_RL.EST\",\"s_RL.EST\",\"s_VA.EST\"] \n",
    "# define a list which stores the results from the prediction\n",
    "prediction_score = []\n",
    "# for loop to run all countries that the user defines in the upper part\n",
    "for i in countries:\n",
    "    # call the upper definded function to prepare the data set for the prediction\n",
    "    real,ref_pre_data = load_ref_data_predict(i)\n",
    "    # define the training data set \n",
    "    descriptors = ref_pre_data[columns]\n",
    "    # and the targt fo the  machine learning\n",
    "    target = ref_pre_data[[\"asylum_seekers\"]]\n",
    "\n",
    "\n",
    "\n",
    "    np.random.seed(seed=12345) # get allways the same random generator\n",
    "    msk = np.random.rand(len(descriptors)) <= 0.85 # mask to split the dataset into the training and test set\n",
    "    # for 10 fold cross validation we would need to make more splits like 0.95, 0.85, 0.75, ...\n",
    "    training_data = descriptors[msk]\n",
    "    test_data = descriptors[~msk]\n",
    "    training_target = target[msk]\n",
    "    test_target = target[~msk]\n",
    "\n",
    "    # Linear Regression\n",
    "    # I am using regression, because I have to predict a real number / integer which is not possible using the classification methods\n",
    "\n",
    "    reg = LinearRegression().fit(training_data, training_target) # learn how the data looks like\n",
    "    #logger.debug(reg.score(training_data,training_target)) # predict and evaluate\n",
    "    reg.predict(test_data) # predict\n",
    "    #logger.info(reg.score(test_data,test_target)) #evaluate\n",
    "\n",
    "    #Lasso Regression\n",
    "\n",
    "    #make parameters list\n",
    "    normalize = [False, True]\n",
    "    intercept = [True, False]\n",
    "    alpha = [1e-15, 1e-10, 1e-5, 1, 5, 10]\n",
    "    results = [] # to collect the results\n",
    "\n",
    "    #loop through all possible parameter combinations\n",
    "    for n in normalize:\n",
    "        for inter in intercept:\n",
    "            for a in alpha:\n",
    "                result_row = {}\n",
    "\n",
    "                result_row[\"normalize\"] = n\n",
    "                result_row[\"intercept\"] = inter\n",
    "                result_row[\"alpha\"] = a\n",
    "\n",
    "                lasso = Lasso(alpha=a, fit_intercept=inter, normalize=n) # create the regressor\n",
    "\n",
    "                lasso.fit(training_data, training_target)\n",
    "                predicted = lasso.predict(test_data)\n",
    "\n",
    "                result_row[\"score\"] = round(lasso.score(test_data,test_target[\"asylum_seekers\"]), 4)\n",
    "                results.append(result_row)\n",
    "    #logger.info(pd.DataFrame(results)) # look over the result list to find the best parameters for machine learning\n",
    "\n",
    "\n",
    "    # Linear Regression revisited - as for the Lasso Regression\n",
    "    normalize = [False, True]\n",
    "    bias = [False, True]\n",
    "    interaction = [False, True]\n",
    "    intercept = [True, False]\n",
    "    degree = [ 1, 2, 3, 4, 5]\n",
    "    results = []\n",
    "    for n in normalize:\n",
    "        for b in bias:\n",
    "            for interact in interaction:\n",
    "                for inter in intercept:\n",
    "                    for d in degree:\n",
    "\n",
    "                        result_row = {}\n",
    "                        result_row[\"normalize\"] = n\n",
    "                        result_row[\"bias\"] = b\n",
    "                        result_row[\"interaction\"] = interact\n",
    "                        result_row[\"intercept\"] = inter\n",
    "                        result_row[\"degree\"] = d\n",
    "\n",
    "                        poly = PolynomialFeatures(include_bias=b, interaction_only=interact, degree=d)\n",
    "                        lrm = LinearRegression(normalize=n, fit_intercept=inter)\n",
    "                        X_poly=poly.fit_transform(training_data)\n",
    "                        X_p_poly=poly.fit_transform(test_data)\n",
    "                        lrm.fit(X_poly, training_target)\n",
    "                        result_row[\"score\"] = round(lrm.score(X_p_poly,test_target[[\"asylum_seekers\"]]), 4)\n",
    "                        results.append(result_row)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    #logger.info(pd.DataFrame(results))\n",
    "    logger.info(pd.DataFrame(results)[[\"score\"]].max())\n",
    "    prediction_score.append(i)\n",
    "    prediction_score.append(pd.DataFrame(results)[[\"score\"]].max())\n",
    "    # use the influenza predict to predict\n",
    "    test_descriptors = real[columns]\n",
    "    test_target = real[[\"asylum_seekers\"]]\n",
    "\n",
    "    poly = PolynomialFeatures(include_bias=False, interaction_only=False, degree=2)\n",
    "    lrm = LinearRegression(normalize=True, fit_intercept=True)\n",
    "\n",
    "    # use the full influenza to learn\n",
    "    X_poly=poly.fit_transform(descriptors)\n",
    "    X_p_poly=poly.fit_transform(test_descriptors)\n",
    "    #logger.info(test_descriptors)\n",
    "    #logger.info(descriptors)\n",
    "    lrm.fit(X_poly, target)\n",
    "    #logger.info(lrm.predict(X_p_poly))\n",
    "    #logger.info(real[\"asylum_seekers\"])\n",
    "    logger.info(round(lrm.score(X_p_poly,test_target[\"asylum_seekers\"]), 4)) # look how it performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "- Can countries that will produce large numbers of refugees be predicted?\n",
    "\n",
    "We have to predicted the flow of refugees for some countries. To do this, we used the 2000-2016 data to predict 2017. We see that it is not possible to predict the data well for theese countrys. The best score for a prediction was  below 40 percent. We used linear regression to predict the data. We have selected the countries with the highest number of refuges and are trying to predict the dates.\n",
    "\n",
    "- Can refugee flows be predicted?\n",
    "\n",
    "On the other hand, it is not so easy to predict the flow of refugees from country to country. For example, if a war breaks out during this time, the refuggie flow is constantly increasing and this fact makes prediction difficult.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
